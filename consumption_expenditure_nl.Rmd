---
title: "Consumption Expenditure of Households in The Netherlands"
author: "Safeen Ghafour"
date: "12/25/2020"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
    number_sections: yes
subtitle: 'HarvardX PH125.9x - Data Science: Capstone'
theme: readable
---

```{r, echo=FALSE, message=FALSE}
# Install packages if necessary

installed.packages <- rownames(installed.packages())

if(("tidyverse" %in% installed.packages) == FALSE) {
  install.packages("tidyverse")
}

if(("kableExtra" %in% installed.packages) == FALSE) {
  install.packages("kableExtra")
}

if(("TSstudio" %in% installed.packages) == FALSE) {
  install.packages("TSstudio")
}

if(("fpp2" %in% installed.packages) == FALSE) {
  install.packages("fpp2")
}

if(("tseries" %in% installed.packages) == FALSE) {
  install.packages("tseries")
}

# includes ggplot2, dplyr
library(tidyverse)
library(kableExtra)
library(fpp2)
library(TSstudio)
library(tseries)

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

For the purpose of this projects we will conduct time series analysis to extract meaningful statistics and insights of the data. Thereafter we will try to forecast future values using various models based on historical observations.

Specifically, we will use the publicly available data of the "Consumption expenditure of households in The Netherlands" from January 2000 to October 2020 for our analysis and forecasting.

Time series analysis and forecasting are widely used in economics, management, production and planning.

Forecasting time series can be challenging due to the vast number of factors that can affect such action and make it highly sensitive. Many irregularities and interruptions may require retraining or even remodelling. This is specially true, in case of irregularities caused unexpected events like the COVID pandemic

Therefore, forecasting is the art of understanding uncertainty. 



# Objectives

We have two main objectives, using the Value Index of the expenditure of households:

1. Data analysis: to better understand various aspects of our data.

2. Forecasting: using different models to predict an unseen subset of data and evaluate and compare their outcomes.



# The Data

The expenditures of households dataset is publicly available from the website of 'Het Centraal Bureau voor de Statistiek' (Central Agency for Statistics - The Netherlands). ^[https://opendata.cbs.nl]

This dataset provides figures on the expenditures of households in values and volume. The figures are divided in domestic consumption and final consumption by households. This includes final consumption in the Netherlands by residents and non-residents.

Available from 2000, this dataset contains changes and indices of consumption of households by type of goods and services. ^[ Licensed under Attribution 4.0 International (CC BY 4.0) by https://www.cbs.nl ]

```{r consumption expenditure data, echo=FALSE}

# Consumption Expenditure of Households
consumption <- read.csv("https://raw.githubusercontent.com/safeenghafour/ConsumptionNL/master/82608ENG_UntypedDataSet_26122020_004554.csv", 
                        sep = ";", 
                        header = TRUE, 
                        stringsAsFactors = FALSE)
str(consumption)

```


The original dataset has 4942 observations and 9 variables.

To better understand the data, the The Central Agency for Statistics provides a meta-data file available from the following URL:
[Consumption Expenditure of Households in The Netherlands - Metadata.](https://opendata.cbs.nl/statline/portal.html?_la=en&_catalog=CBS&tableId=82608ENG&_theme=1051)


```{r, echo=FALSE}

# Display the metadat in a table

column_name <- c('ID', 'ConsumptionByHouseholds', 'Periods','Indices_1', 'VolumeChanges_2', 'VolumeChangesShoppingdayAdjusted_3', 'Indices_4', 'ValueChanges_5', 'PriceChanges_6')
column_exp <- c('Index', 'Consumption By Households is a nested category of products and services.', 'Date separated by Year, by Quarter and by Month.', 'Volume Indices: An index represents the ratio between the value of a certain variable in a certain period and the value of that same variable in the base period.', 'Volume Changes: The change of volume compared to the same period a year earlier.', 'Volume Changes Shoppingday Adjusted: For shopping day adjusted change of volume compared to the same period a year earlier.', 'Value Indices: An index represents the ratio between the value of a certain variable in a certain period and the value of that same variable in the base period.', 'Value Changes: The change of value compared to the same period a year earlier.', 'Price Changes: The change of price compared to the same period a year earlier.')

explanation <- data.frame(column_name, column_exp)

kbl(explanation, booktabs = T) %>%
kable_styling(latex_options = "striped", full_width = F) %>%
column_spec(1, bold = T, color = "#4285f4") %>%
column_spec(2, width = "30em")

```



## The variables


***Periods***

The original data starts from January 2000 to October 2020.

There are three different types of periods/dates in this variable which are separated by one of the delimiters:

MM: Monthly data

KW: Quarterly data

JJ: Yearly data


***ConsumptionByHouseholds***

Consumption by households is a nested category. We have added a sequence number to the category list:

```{r, echo=FALSE}

Key <- c('A047812', 'A047813', 'A047875', 'A047825', 'A047826', 'A047827', 'A047828', 'A047829', 'A047830', 'A047831', 'A048214', 'A047832', 'A048213', 'A047837')
Name <- c(
  '1 Domestic consumption by households', 
  '1.1 Consumption of goods by households', 
  '1.1.1 Foodproducts, beverages and tabacco', 
  '1.1.2 Durable consumer goods', 
  '1.1.2.1 Textiles and clothing', 
  '1.1.2.2 Leather goods and footwear', 
  '1.1.2.3 Home furnishing and home decoration', 
  '1.1.2.4 Electrical equipment', 
  '1.1.2.5 Vehicles', 
  '1.1.2.6 Other durable consumer goods n.e.c.', 
  '1.1.3 Other goods', 
  '1.1.4 Electricity, gas, water and motor fuels', 
  '1.1.5 Personal care and other goods', 
  '1.2 Consumption of services by households')

categories <- data.frame(Key, Name)

kbl(categories, booktabs = T) %>%
kable_styling(latex_options = "striped", full_width = F) %>%
column_spec(1, bold = T, color = "#4285f4") %>%
column_spec(2, width = "30em")

```


***Indices_1*** & ***Indices_4***

Both Volume and the Value Indices are considered to be 100 in ***2015**, which is the base period. All historical and future values of the indices are ratios of the base period values.

Value and volume are concepts used in publications about the economy. However, volume is often mistaken for quantity, which is just one of the three components that make up volume. For more information we refer the reader to this article [The volume concept in economic publications](https://www.cbs.nl/en-gb/our-services/methods/surveys/aanvullende-onderzoeksbeschrijvingen/the-volume-concept-in-economic-publications).


***VolumeChanges_2*** & ***ValueChanges_5***

The change of volume and value compared to the same period a year earlier.


***VolumeChangesShoppingdayAdjusted_3*** 

Adjustment for the number of shopping days in a period. This data is incomplete.


***PriceChanges_6***

The change of price compared to the same period a year earlier.



# Data analysis

We will base our analysis on the value index variable (Indices_4) of the expenditure of households. 

Therefore, we need to modify the dataset to fit our purpose.

First we grab ***monthly data*** from the dataset and extract year and month columns and add labelled category, then we filter out the other variables.

```{r, echo=FALSE}

# Date preparation, extract year and month columns, rename columns to more meaningful names
consumption_modified <- consumption %>%
  filter( grepl("MM", Periods) ) %>%
  mutate(
    date = as.Date(paste(gsub("MM", "-", Periods), "-01", sep = "")),
    year = as.factor(substr(Periods, 1, 4)),
    month = as.factor(substr(Periods, 7, 8)),
    cvalue = as.numeric(Indices_4)) %>%
  rename(
    category_id = ConsumptionByHouseholds,
    consumption_value = cvalue
  ) %>%
  summarise(category_id, year, month, consumption_value)

# label the levels
consumption_modified$category <- 
  factor(consumption_modified$category_id, 
levels = Key,
labels = Name
)

str(consumption_modified)

head(consumption_modified)

```

The modified dataset has 3500 observations and 5 variables.


We plot the top category 'Domestic consumption by households' and its two major sub-categories 'Consumption of goods by households' & 'Consumption of services by households'.

The value of 'Domestic consumption by households' is the average value of its two sub-categories and their sub-subs. Logically the lines come together in 2015, when all the values for all individual categories are were set to be a 100 index.

Note: The average values of 2020 are divided by 10 months.

```{r, echo=FALSE, message=FALSE}

# plot the three major categories
consumption_modified %>%
  filter(category_id == "A047812" | category_id == "A047837" | category_id == "A047813" ) %>%
  group_by(year, category) %>%
  arrange(category_id) %>%
  summarise(val = sum(consumption_value)/n()) %>%   
  ggplot(aes(x=year, y=val, color=str_wrap(category, 18), group=category)) +
  theme_light(base_size = 11) +
  xlab("Year") +
  ylab("Value") +
  labs(colour="") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position="bottom", plot.caption = element_text(color = "#568bc1")) +
  geom_line() +
  labs(caption = "Yearly consumption: Goods & Services") 

```

For the purpose of illustration only we fill the last two missing months of 2020, to be very optimistic, with a lag of 12 months.
Not to be used in calculations hereafter.

```{r, echo=FALSE, message=FALSE}

# Add November and December values to 2020

# loop categories
for(i in Key) {
  consumption_modified_filled <- consumption_modified %>%
    add_row(category_id = i, year = '2020', month = '11') %>%
    add_row(category_id = i, year = '2020', month = '12')
}

# fill the last two months of 2020 with a lag of 12 months
consumption_modified_filled <- consumption_modified_filled %>%
  group_by(category_id) %>%
  mutate(consumption_value = ifelse(year == '2020' & (month == '11' | month == '12'), lag(consumption_value,12), consumption_value) )

# label the levels
consumption_modified_filled$category <- 
  factor(consumption_modified_filled$category_id, 
         levels = Key,
         labels = Name
)

# plot the three major categories
consumption_modified_filled %>%
  filter(category_id == "A047812" | category_id == "A047837" | category_id == "A047813" ) %>%
  group_by(year, category) %>%
  arrange(category_id) %>%
  summarise(val = sum(consumption_value)/n()) %>%   
  ggplot(aes(x=year, y=val, color=str_wrap(category, 18), group=category)) +
  theme_light(base_size = 11) +
  xlab("Year") +
  ylab("Value") +
  labs(colour="") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position="bottom", plot.caption = element_text(color = "#568bc1")) +
  geom_line() +
  labs(caption = "Yearly consumption: Goods & Services ~ 2000-01 - 2020-12")

```

The effect is not that significant and no matter how good the recovery is in the last two months of 2020, the damage is already done.

Both Goods and Services are dependent on a good performing economy. 

During the 2009 financial crises both categories are almost equally effected. However, the fluctuations depend on the type of crisis as clear during the COVID pandemic.^[https://en.wikipedia.org/wiki/Great_Recession]

The consumption of Goods in 2020 is far less effected than the consumption of Services by COVID and lock-down. Services consumption, that includes Housing, Hotel, Recreational, Transport and Communication, Medical, Financial and Business services, has been hit hard. 


We will continue with our original dataset and further zoom into the five major sub-categories of Goods.

```{r, echo=FALSE, message=FALSE}

# plot the three major categories
consumption_modified %>%
  filter(category_id == "A047875" | category_id == "A047825" | category_id == "A048214" | category_id == "A048213" | category_id == "A047832") %>%
  group_by(year, category) %>%
  summarise(val = sum(consumption_value)/n()) %>%   
  ggplot(aes(x=year, y=val, color=str_wrap(category, 14), group=category)) + 
  theme_light(base_size = 11) +
  xlab("Year") +
  ylab("Value") +
  labs(colour="") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position="bottom",plot.caption = element_text(color = "#568bc1")) +
  geom_line() +
  labs(caption = "Year consumption of Goods: breakdown") 

```

This plot makes the effect of COVID even more clear. The consumption of 'Electricity, gas, water and motor fuels' and 'Other goods' are crashed while 'Foodproducts, beverages and tobacco' show a growth compared with previous years.


Now we plot 'Durable consumer goods'.

```{r, echo=FALSE, message=FALSE}

# durable consumer goods
consumption_modified %>%
  filter(category_id == "A047826" | category_id == "A047827" | category_id == "A047828" | category_id == "A047829" | category_id == "A047830" | category_id == "A047831") %>%
  group_by(year, category) %>%
  summarise(val = sum(consumption_value)/n()) %>%   
  ggplot(aes(x=year, y=val, color=str_wrap(category, 14), group=category)) + 
  theme_light(base_size = 11) +
  xlab("Year") +
  ylab("Value") +
  labs(colour="") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position="bottom", plot.caption = element_text(color = "#568bc1")) +
  geom_line() +
  labs(caption = "Year consumption of Durable consumer goods: breakdown" )

```

Here we notice a decline in the consumption of 'Vehicles', 'Textiles and clothing' and 'Leather goods and footwear'. Obviously, people had enough time to repair and decorate their houses.

To give an idea about the size of the expenditure in Euro's we refer you to [Consumption by type of goods and services; National Accounts](https://opendata.cbs.nl/#/CBS/en/dataset/84093ENG/table?searchKeywords=prices).



# Time series nanalysis

In this section we will focus on the structure of our time series dataset which is a monthly time series data starting from November 2000 to October 2020.

The autocorrelation is a characteristic of time series, one observation depends on previous observations.

In the following analysis we try to identify patterns in our dataset based on the four major components trend, cycle, seasonality, and irregularity.

First we convert our dataset a time series object.

```{r, echo=FALSE, message=FALSE}

# Choose top category 'Domestic consumption by households'
general_consumption <- consumption_modified %>%
    filter(category_id == "A047812")
  
# Create time series object
general_consumption_ts <- ts(
  general_consumption$consumption_value,
  start = c(2000, 1),
  end = c(2020, 10),
  frequency = 12
)

ts_info(general_consumption_ts)

```


## Trend

It was already clear from "Figure: Domestic consumption: Goods & Services" that the series show an exponential trend upwards.


```{r, echo=FALSE, message=FALSE}

# plot trend component
plot(general_consumption_ts, col='coral1', xlab="Year", ylab="Value", main = "")
grid (14, 10, lty = 1, col = "#dddddd") 
title(main = "", sub = "Trend component", cex.sub = 0.75, font.sub = 1, col.sub = "#568BC1", adj = 1)

```


## Cycle

The same plot above shows no signs of clear cycles, that is regular ups and downs in the trend.


## Seasonal

The presence of variations that occur at specific regular intervals less than a year in our case.
To examine seasonality we will plot the values grouped by months.


```{r, echo=FALSE, message=FALSE}

# plot seasonal component
boxplot(general_consumption_ts~cycle(general_consumption_ts), xlab = "Year", ylab = "Value", col = "#fff9f9", main="")
title(main = "", sub = "Seasonal component", cex.sub = 0.75, font.sub = 1, col.sub = "#568BC1", adj = 1)

```

Lowest value is in February while the highest is in December.


## Irregular

This component, which is the remainder between the series and its structural components, provides an indication of irregular events in the series.

There are two irregularities, which are not cycles, obvious in our dataset, a less strong one in 2009 and the second in 2020.


Finally we will decompose our time series dataset isolating each of the above mentioned patterns.


```{r, echo=FALSE, message=FALSE}

# plot the decomposition
plot(decompose(general_consumption_ts), xlab="Year", col="coral1")
title(main = "", sub = "Decomposition", cex.sub = 0.75, font.sub = 1, col.sub = "#568BC1", adj = 1)

```


## Autocorrelation

Autocorrelation is a correlation of a series with itself (own lagged values), which means it dependence on its past values.

We will measure and plot the correlation between the series and its lags using the autocorrelation function (ACF). 

The values that cross the dashed blue lines (statistical significance), mean that the correlation significance of the lag with current series.

We will also use partial autocorrelation function (PACF), that is the amount of correlation between a time series and lags of itself that is not explained by a previous lag (residuals).



```{r, echo=FALSE, message=FALSE}

# plot ACF
acf(general_consumption_ts, col="coral1", main = "")
title(main = "", sub = "ACF", cex.sub = 0.75, font.sub = 1, col.sub = "#568BC1", adj = 1)

# plot pacf
pacf(general_consumption_ts, col="coral1", main = "")
title(main = "", sub = "PACF", cex.sub = 0.75, font.sub = 1, col.sub = "#568BC1", adj = 1)


```

The correlation of the series with its lags is decaying over time and chronologically closer lags to the series show a stronger relation.


## Stationarity

Many time series models require the data to be stationary.

A time series is stationary if its mean, variance and covariance remain constant over the whole series. 

We can test stationarity by looking at the decomposition plot. Both Trend and Seasonal components are good indications that our time series is not stationary.

Another way to perform such test using the ACF. The strong relation between the series and its lags is another indicator.

Finally we will use Dickey-Fuller hypothesis testing.
The Null Hypothesis: The series is not stationary.

```{r, echo=FALSE}

# Dickey Fuller Test
print(adf.test(general_consumption_ts))

```

With the p-value greater than 0.05, we fail to reject the null hypothesis & confirm that the series is "not stationary".


To fix this we use differencing. Differencing is the process of subtracting one observation from another.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# Dickey Fuller Test on diff
print(adf.test(diff(general_consumption_ts, differences=1)))

```

With the p-value less than 0.05, we can now reject the null hypothesis and confirm that the series is stationary.



# Forecasting

The second objective of this project is to forecast using different models to predict an unseen subset of data and evaluate and compare the outcomes.

First, We will split the dataset into two sequencial partitions, leaving the last 60 observations of the series as the
testing partition and the rest as training. 


```{r, echo=FALSE, message=FALSE}

# test partintion size
p <- 60

# split the main dataset
general_consumption_split <- ts_split(ts.obj = general_consumption_ts, sample.out = p)
train <- general_consumption_split$train
test <- general_consumption_split$test

# training set
ts_info(train)


# testing set
ts_info(test)

```



## Scoring

Common methods for evaluating the success of the forecast in order to predict the actual values, are accuracy or error metrics.

We will use the Root Mean Squared Error (RMSE) and the Mean Absolute Percentage Error (MAPE).

RMSE = The root mean of all the squared differences between the actual value at a certain time and the forecasted value at the same time.

MAPE = The mean of the absolute differences between the actual value at a certain time and the forcasted value at the same time divided by the actual value at the same time then multiplied by a hundred.



## Benchmarking

To asses the results we obtained from RMSE and MAPE we will benchmark the model's performance to a baseline forecast. 

We use naive forecast, that is the most recently observed value aplied to all predictions.

***Naive forecast***

```{r, echo=FALSE, warning=FALSE, message=FALSE}

# Forecast 60 months, the same period as the test partition
h <- 60

# Navie model for benchmarking
naive_model <- naive(train, h = h)
# The model's forecast
naive_forecast <- forecast(naive_model, h)
# The model's accuracy
naive_accuracy <- forecast::accuracy(naive_forecast, test)
# Print RMSE & MAPE
results <- tibble(Method = "Naive", RMSE = naive_accuracy[4], MAPE = naive_accuracy[10])

results

# We collect the forecasted values for plotting
test_frame <- ts_to_prophet(test)
test_frame$ds <- substr(test_frame$ds, 1, 7)
test_frame$type <- as.factor("Test")
names(test_frame) <- c("YM", "Value", "Type")

naive_frame <- ts_to_prophet(naive_forecast$mean)
naive_frame$ds <- substr(naive_frame$ds, 1, 7)
naive_frame$type <- as.factor("Naive")
names(naive_frame) <- c("YM", "Value", "Type")

forecast_frame <- rbind(test_frame, naive_frame)


forecast_frame %>%
  ggplot(aes(x = YM, y = Value, color=str_wrap(Type, 14), group=Type)) +
    theme_light(base_size = 11) +
    xlab("Year - Month") +
    ylab("Value") +
    labs(colour="") +
    theme(axis.text.x = element_blank(), axis.ticks.x=element_blank(), legend.position="bottom", plot.caption = element_text(color = "#568bc1")) +
    geom_line() +
  labs(caption = "Compared prediction results")

```

We will compare this data to evaluate performance, the lower the better.
Our base RMSE is 8.77 and base MAPE is 6.55. If any other models scores lower than this we consider it better.

From the above plot we can clearly see the straight Naive line starting from the last value of the training set.



## Holt's Trend

This is an extension of the simple exponential smoothing method, also known as linear exponential smoothing, which considers the trend component while generating forecasts. This method involves two smoothing equations, one for the level and one for the trend component.

```{r, echo=FALSE, warning=FALSE, message=FALSE}

# Holt's Trend
holt_model <- holt(train, h = h)
holt_forecast <- forecast(holt_model, h)
holt_accuracy <- forecast::accuracy(holt_forecast, test)

results <- results %>%
  add_row(Method = "Holt", RMSE = holt_accuracy[4], MAPE = holt_accuracy[10])
results

# We collect the forecasted values for plotting
holt_frame <- ts_to_prophet(holt_forecast$mean)
holt_frame$ds <- substr(holt_frame$ds, 1, 7)
holt_frame$type <- as.factor("Holt")
names(holt_frame) <- c("YM", "Value", "Type")

forecast_frame <- rbind(forecast_frame , holt_frame)

forecast_frame %>%
  ggplot(aes(x = YM, y = Value, color=str_wrap(Type, 14), group=Type)) +
    theme_light(base_size = 11) +
    xlab("Year - Month") +
    ylab("Value") +
    labs(colour="") +
    theme(axis.text.x = element_blank(), axis.ticks.x=element_blank(), legend.position="bottom", plot.caption = element_text(color = "#568bc1")) +
    geom_line() +
  labs(caption = "Compared prediction results")

```

The results of Holt's Trend look much better than our Naive benchmark and the plot follows an upward trend. 



## ARIMA

One of the most common methods used in time series forecasting is known as the ARIMA model, which stands for Auto Regressive (AR) Integrated (I) Moving Average (MA). 

It is based on the existence of correlation between a time series and its lags.

AR(p)
Autoregressive model is based on the idea that the current value can be explained as a function of past p values, where p is the number of steps into the past (lag) needed to forecast the current model. 

I(d)
A function to make a time series stationary using differencing where d is the number of differencing transformations required by the time series to become stationary.

MA(q)
moving average model uses past forecast errors in a regression-like model where q is order; past error (multiplied by a coefficient).


Since we use auto.arima function, the values of p, d and q will be calculated automatically.


```{r, message=FALSE, echo=FALSE}

# ARIMA Model
# (Takes some time)
arima_model <- auto.arima(train, stepwise = FALSE, approximation = FALSE, seasonal=TRUE)
arima_forecast <- forecast(arima_model, h)
arima_accuracy <- forecast::accuracy(arima_forecast, test)

results <- results %>%
  add_row(Method = "ARIMA", RMSE = arima_accuracy[4], MAPE = arima_accuracy[10])
results

# We collect the forecasted values for plotting
arima_frame <- ts_to_prophet(arima_forecast$mean)
arima_frame$ds <- substr(arima_frame$ds, 1, 7)
arima_frame$type <- as.factor("Auto ARIMA")
names(arima_frame) <- c("YM", "Value", "Type")

forecast_frame <- rbind(forecast_frame , arima_frame)

forecast_frame %>%
  ggplot(aes(x = YM, y = Value, color=str_wrap(Type, 14), group=Type)) +
    theme_light(base_size = 11) +
    xlab("Year - Month") +
    ylab("Value") +
    labs(colour="") +
    theme(axis.text.x = element_blank(), axis.ticks.x=element_blank(), legend.position="bottom", plot.caption = element_text(color = "#568bc1")) +
    geom_line() +
    labs(caption = "Compared prediction results") 

```

Now the RMSE and MAPE have decreased and the plot depicts that the model picks the Trend and Seasonality. However the unexpected effect of the COVID lock-down is pretty obvious.

Now we will check the residuals of the ARIMA model.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# check residuals of ARIMA Model
checkresiduals(arima_forecast)

```

Obviously, p equals 3 and with 1 differencing and q is 0, which means no moving averages.  

***Ljung-Box test***

The p-values for the Ljung–Box statistics are small, indicating there is some pattern in the residuals. The p-value is less than 0.05, which means that we cannot reject the null hypothesis that there is no autocorrelation is left.


***ACF***

The ACF shows significant autocorrelations between residuals and the model did not fully capture all of the patterns.



## Neural Network AutoRegression

Artificial neural networks are forecasting methods that are based on simple mathematical models of the brain. They allow complex nonlinear relationships between the response variable and its predictors.

In the Neural Network AutoRegression (NNAR) the lagged values of the time series is used as inputs to a neural network.


```{r, echo=FALSE, warning=FALSE, message=FALSE}

# Neural Network AutoRegression Model (NNAR)
set.seed(1973)
# λ=0.5 to ensure the residuals will be roughly homoscedastic.
# 4 hidden layers
nn_model <-  nnetar(train, lambda = 0.5, repeats = 40, size = 4)
nn_forecast <- forecast(nn_model, h)
nn_accuracy <- forecast::accuracy(nn_forecast, test)

results <- results %>%
  add_row(Method = "Neural Network", RMSE = nn_accuracy[4], MAPE = nn_accuracy[10])
results

# We collect the forecasted values for plotting
nn_frame <- ts_to_prophet(nn_forecast$mean)
nn_frame$ds <- substr(nn_frame$ds, 1, 7)
nn_frame$type <- as.factor("Neural Network")
names(nn_frame) <- c("YM", "Value", "Type")

forecast_frame <- rbind(forecast_frame , nn_frame)

forecast_frame %>%
  ggplot(aes(x = YM, y = Value, color=str_wrap(Type, 14), group=Type)) +
    theme_light(base_size = 11) +
    xlab("Year - Month") +
    ylab("Value") +
    labs(colour="") +
    theme(axis.text.x = element_blank(), axis.ticks.x=element_blank(), legend.position="bottom", plot.caption = element_text(color = "#568bc1")) +
    geom_line() +
  labs(caption = "Compared prediction results") 

```


The RMSE and MAPE are slightly improved and again the plot shows that the model picks the Trend and Seasonality.

Now we will check the residuals of the Neural Network model.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

# Check residuals of the NNAR Model
Box.test(nn_forecast$residuals, lag = 4, type="Ljung-Box") 
checkresiduals(nn_forecast)

```

The p-value in the ***Ljung-Box test*** is still not greater than 0.05 and the ***ACF*** looks a bit better than the ARIMA ACF of residuals, but still patterns to catch.



## Other model

There are many other models to consider such as KNN, Bootstrapping and bagging, Prophet, GARCH, XGBoost, etc. 
However, we think we have covered the important ones using traditional and neural networks.

## The future

We will use the trained ARIMA and NN models to predict 24 months beyond the test subset.

```{r, echo=FALSE, message=FALSE}

# ARIMA MODEL
arima_future <- forecast(arima_model, (h + 12) )

# We collect the forecasted values for plotting
arima_future_frame <- ts_to_prophet(arima_future$mean)
arima_future_frame$ds <- substr(arima_future_frame$ds, 1, 7)
arima_future_frame$type <- as.factor("ARIMA")
names(arima_future_frame) <- c("YM", "Value", "Type")

# NN MODEL
nn_future <- forecast(nn_model, (h + 12) )

# We collect the forecasted values for plotting
nn_future_frame <- ts_to_prophet(nn_future$mean)
nn_future_frame$ds <- substr(nn_future_frame$ds, 1, 7)
nn_future_frame$type <- as.factor("Neural Network")
names(nn_future_frame) <- c("YM", "Value", "Type")

forecast_future_frame <- rbind(arima_future_frame, nn_future_frame)

forecast_future_frame %>%
  ggplot(aes(x = YM, y = Value, color=str_wrap(Type, 14), group=Type)) +
    theme_light(base_size = 9) +
    xlab("Year - Month") +
    ylab("Value") +
    labs(colour="") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), axis.ticks.x=element_blank(), legend.position="bottom", plot.caption = element_text(color = "#568bc1")) +
    geom_line() +
  geom_vline(xintercept = 60, linetype="dotted", color = "blue", size= 0.5) +
  labs(caption = "The future") 

```

The NNAR model flattens a bit while ARIMA looks to have a more logical distribution.
This is a good indication that continous retraining is necessary.

# Conclusion

From the models we have used the "Neural Network AutoRegression" has shown the best results for the chosen criteria.

We have also noticed the limitations of time series models in dealing with sudden irregularities like COVID.

Most importantly, training a time series model is a never ending process. Whenever new data become available, the model needs retraining.


- - - - - - - - -
